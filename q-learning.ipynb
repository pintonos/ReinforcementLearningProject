{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available  \n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Set a seed for reproducing results\n",
    "random_seed = 1234\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# PyTorch specific things\n",
    "# Check https://pytorch.org/docs/stable/notes/randomness.html for more on this\n",
    "torch.manual_seed(random_seed)\n",
    "if cuda_available:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1234]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "# By default, the max steps for this environment is 200\n",
    "# which is difficult for the simple algorithms we have used till now\n",
    "# To make things easier, we increase the max steps per episode\n",
    "env._max_episode_steps = 4000\n",
    "\n",
    "# Set the same random seed in gym for reproducing results\n",
    "env.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Action space: Discrete(3)\nSample action: 0\nType of action: <class 'int'>\nObservationt state space: Box(-28.274333953857422, 28.274333953857422, (6,), float32)\nShape of state space: (6,)\nInitial state: [ 0.99965389 -0.02630794  0.99678118 -0.08017036  0.0960371  -0.00889859]\nSample observation: [ 0.7354413   0.36610872  0.41524902 -0.90963316  8.268611   13.631855  ]\nType of observation: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# What is the type and size of the action space\n",
    "print(\"Action space: {}\".format(env.action_space))  # 3 discrete actions, \n",
    "\n",
    "# What does an action look like\n",
    "sample_action = env.action_space.sample()  # Action can be one of these: 0, 1, 2\n",
    "print(\"Sample action: {}\".format(sample_action))  # Execute multiple times to see different actions\n",
    "print(\"Type of action: {}\".format(type(sample_action)))\n",
    "\n",
    "# What is the type and size of the observation (state) space\n",
    "print(\"Observationt state space: {}\".format(env.observation_space))  # Continuous 2D states\n",
    "print(\"Shape of state space: {}\".format(env.observation_space.shape))  # Continuous 2D states\n",
    "\n",
    "# Which state does the agent start in?\n",
    "initial_state = env.reset()\n",
    "print(\"Initial state: {}\".format(initial_state))  \n",
    "\n",
    "# What is an observation\n",
    "sample_observation = env.observation_space.sample()\n",
    "print(\"Sample observation: {}\".format(sample_observation))\n",
    "print(\"Type of observation: {}\".format(type(sample_observation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected neural network with 1 hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(Network, self).__init__()\n",
    "        self.env = env\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        self.hidden = 200\n",
    "        self.l1 = nn.Linear(self.state_space, self.hidden, bias=False)\n",
    "        self.l2 = nn.Linear(self.hidden, self.action_space, bias=False)\n",
    "    \n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.ReLU(),\n",
    "            self.l2\n",
    "        )\n",
    "        return model(x)\n",
    "\n",
    "    def save(self, save_dir=\"models\", file_name=\"q_network.pt\"):\n",
    "        # Save the model state\n",
    "        if not (os.path.exists(save_dir) and os.path.isdir(save_dir)):\n",
    "            os.makedirs(save_dir)\n",
    "        torch.save(self.state_dict(), os.path.join(save_dir, file_name))\n",
    "\n",
    "    @staticmethod\n",
    "    def load(env, save_dir=\"models\", file_name=\"q_network.pt\"):\n",
    "        # Create a network object with the constructor parameters\n",
    "        network = Network(env)\n",
    "        # Load the weights\n",
    "        network.load_state_dict(torch.load(os.path.join(save_dir, file_name)))\n",
    "        # Set the network to evaluation mode\n",
    "        network.eval()\n",
    "        return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Agent:\n",
    "    \"\"\"\n",
    "    A Q-Learning agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, num_episodes, num_steps, learning_rate, gamma, epsilon):\n",
    "        \"\"\"\n",
    "        Contructor\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.q_network = Network(env=env)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\n",
    "        self.criteria = nn.MSELoss()\n",
    "       \n",
    "    def random_action(self):\n",
    "        \"\"\"\n",
    "        Select a random action.\n",
    "        \"\"\"\n",
    "        return self.env.action_space.sample()\n",
    "       \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Implementation of the Q-Learning algorithm using a neural network as the function approximator\n",
    "        for the action-value function.\n",
    "        \n",
    "        :returns: episode_rewards: Array containg the cumulative reward in each episode\n",
    "        \"\"\"\n",
    "        # Array to store cumulative rewards per episode\n",
    "        episode_rewards = np.zeros((self.num_episodes, 1))\n",
    "\n",
    "        best_episode_reward = -np.inf\n",
    "        \n",
    "        # For each episode\n",
    "        for episode in trange(self.num_episodes):\n",
    "            \n",
    "            # Cumulative reward\n",
    "            episode_reward = 0\n",
    "            episode_loss = 0\n",
    "                \n",
    "            # Initialize the environment and get the first state\n",
    "            state = env.reset()\n",
    "            \n",
    "            # Set the done variable to False\n",
    "            done = False\n",
    "\n",
    "            # Variable for tracking the time\n",
    "            t = 0\n",
    "                    \n",
    "            # For each step of the episode\n",
    "            while (not done):\n",
    "                \n",
    "                # Use the state as input to compute the q-values (for all actions in 1 forward pass)\n",
    "                q = self.q_network(Variable(torch.from_numpy(state).type(torch.FloatTensor)))\n",
    "                                \n",
    "                # Choose epsilon-greedy action based on q\n",
    "                if np.random.rand(1) < self.epsilon:\n",
    "                    action = self.random_action()\n",
    "                else:\n",
    "                    _, action = torch.max(q, -1)\n",
    "                    # Convert from tensor to float/int\n",
    "                    action = action.item()\n",
    "                    \n",
    "                # Perform the action and observe the next_state and reward\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                                \n",
    "                # Find max q for next state\n",
    "                with torch.no_grad():\n",
    "                    q_next = self.q_network(Variable(torch.from_numpy(next_state).type(torch.FloatTensor)))\n",
    "                    q_next = q_next.detach()\n",
    "                max_q_next, _ = torch.max(q_next, -1)\n",
    "                \n",
    "                # Create target q value for training\n",
    "                q_target = q.clone()\n",
    "                q_target = Variable(q_target.data)\n",
    "                q_target[action] = reward + torch.mul(max_q_next.detach(), self.gamma)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = self.criteria(q, q_target)\n",
    "\n",
    "                # Update policy\n",
    "                self.q_network.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Record history\n",
    "                episode_loss += loss.item()\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # Update the state\n",
    "                state = next_state\n",
    "                \n",
    "                # Increment the timestep\n",
    "                t += 1\n",
    "                \n",
    "                # Exit if the max number of steps has been exceeded\n",
    "                if t>=self.num_steps:\n",
    "                    done = True\n",
    "                    \n",
    "            # Store the cumulative reward for this episode        \n",
    "            episode_rewards[episode] = episode_reward\n",
    "            \n",
    "            # Save the model if this is the best episode till now\n",
    "            if episode_reward>best_episode_reward:\n",
    "                self.q_network.save()\n",
    "\n",
    "        return episode_rewards\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Loads a saved q-network and uses it to run test episodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Load saved network\n",
    "        saved_q_network = Network.load(self.env)\n",
    "        saved_q_network.eval()\n",
    "        \n",
    "        # Array to store cumulative rewards per episode\n",
    "        episode_rewards = np.zeros((self.num_episodes, 1))\n",
    "       \n",
    "        # For each episode\n",
    "        for episode in trange(self.num_episodes):\n",
    "            \n",
    "            # Cumulative reward\n",
    "            episode_reward = 0\n",
    "                \n",
    "            # Initialize the environment and get the first state\n",
    "            state = env.reset()\n",
    "            \n",
    "            # Set the done variable to False\n",
    "            done = False\n",
    "\n",
    "            # Variable for tracking the time\n",
    "            t = 0\n",
    "                    \n",
    "            # For each step of the episode\n",
    "            while (not done):\n",
    "                \n",
    "                q = saved_q_network(Variable(torch.from_numpy(state).type(torch.FloatTensor)))\n",
    "                                \n",
    "                # Choose greedy action based on q\n",
    "                _, action = torch.max(q, -1)\n",
    "                # Convert from tensor to float/int\n",
    "                action = action.item()\n",
    "                    \n",
    "                # Perform the action and observe the next_state and reward\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                # Record history\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # Update the state\n",
    "                state = next_state\n",
    "                \n",
    "                # Increment the timestep\n",
    "                t += 1\n",
    "                \n",
    "                # Exit if the max number of steps has been exceeded\n",
    "                if t>=self.num_steps:\n",
    "                    done = True\n",
    "                    \n",
    "            # Store the cumulative reward for this episode        \n",
    "            episode_rewards[episode] = episode_reward\n",
    "            \n",
    "        return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent = Q_Agent(env, num_episodes=200, num_steps=2000, learning_rate=0.0001, gamma=0.99, epsilon=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1                  [-1, 200]           1,200\n            Linear-2                    [-1, 3]             600\n================================================================\nTotal params: 1,800\nTrainable params: 1,800\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.01\nEstimated Total Size (MB): 0.01\n----------------------------------------------------------------\nNone\n"
     ]
    }
   ],
   "source": [
    "# Check the network structure of the q-network\n",
    "print(summary(q_agent.q_network, input_size=env.observation_space.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Q agent\n",
    "episode_rewards = q_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Agent:\n",
    "    \"\"\"\n",
    "    An agent that always takes random actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, num_episodes, num_steps):\n",
    "        \"\"\"\n",
    "        Contructor\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_steps = num_steps\n",
    "       \n",
    "    def random_action(self):\n",
    "        \"\"\"\n",
    "        Select a random action\n",
    "        \"\"\"\n",
    "        return self.env.action_space.sample()\n",
    "  \n",
    "    def run(self):\n",
    "        # Array to store cumulative rewards per episode\n",
    "        episode_rewards = np.zeros((self.num_episodes, 1))\n",
    "        # For each episode\n",
    "        for episode in trange(self.num_episodes):\n",
    "            # Cumulative reward\n",
    "            episode_reward = 0\n",
    "            # Initialize the environment and get the first state\n",
    "            state = env.reset()\n",
    "            # Set the done variable to False\n",
    "            done = False\n",
    "            # Variable for tracking the time\n",
    "            t = 0\n",
    "            # For each step of the episode\n",
    "            while (not done):\n",
    "                action = self.random_action()\n",
    "                # Perform the action and observe the next_state and reward\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # Record history\n",
    "                episode_reward += reward\n",
    "                # Update the state\n",
    "                state = next_state\n",
    "                # Increment the timestep\n",
    "                t += 1\n",
    "                # Exit if the max number of steps has been exceeded\n",
    "                if t>=self.num_steps:\n",
    "                    done = True\n",
    "            # Store the cumulative reward for this episode        \n",
    "            episode_rewards[episode] = episode_reward\n",
    "        return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = Random_Agent(env, num_episodes=200, num_steps=2000)\n",
    "random_episode_rewards = random_agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(episode_rewards, color=\"blue\", lw=2, label=\"Q-Learning\")\n",
    "plt.plot(random_episode_rewards, color=\"red\", lw=2, label=\"Random\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid()\n",
    "l=plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "dde6d150593e47ea048537f9ba843895d4a64c06f41c23661e670780792c2c3b"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}